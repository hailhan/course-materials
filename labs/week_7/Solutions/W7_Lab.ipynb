{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACS 30113 Lab Session: Working with EMR Clusters/Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today's Lab Agenda: \n",
    "\n",
    "1. presenting two methods to work with Spark\n",
    "2. introduction to some simple pyspark commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create S3 bucket to store our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:57:55.401001Z",
     "start_time": "2024-05-06T22:57:55.391513Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 handler\n",
    "s3 = boto3.resource('s3')\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')\n",
    "\n",
    "# Create a new bucket to store your files\n",
    "BUCKETNAME = 'rei-example-bucket'\n",
    "s3.create_bucket(Bucket=BUCKETNAME)\n",
    "\n",
    "# This is what we will use to interface with the specific bucket\n",
    "bucket = s3.Bucket(BUCKETNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:58:08.783183Z",
     "start_time": "2024-05-06T22:58:08.543064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded to S3 under 'lab_wk7/' folder.\n"
     ]
    }
   ],
   "source": [
    "# Upload the .py file\n",
    "with open('KEY_lab_wk7_spark.py', 'rb') as py_file:\n",
    "    bucket.put_object(Key='lab_wk7/lab_wk7_spark.py', Body=py_file)\n",
    "\n",
    "print(\"Files uploaded to S3 under 'lab_wk7/' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching EMR Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next launch EMR Cluster in Terminal/bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:24:59.258263Z",
     "start_time": "2024-05-06T22:24:56.359573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ClusterId\": \"j-3R36A8AV7QM2I\",\n",
      "    \"ClusterArn\": \"arn:aws:elasticmapreduce:us-east-1:014303519904:cluster/j-3R36A8AV7QM2I\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# ! please remember to change your bucket name\n",
    "aws emr create-cluster \\\n",
    "    --name \"Spark Cluster\" \\\n",
    "    --release-label \"emr-6.2.0\" \\\n",
    "    --applications Name=Hadoop Name=Hive Name=JupyterEnterpriseGateway Name=JupyterHub Name=Livy Name=Pig Name=Spark Name=Tez \\\n",
    "    --instance-type m5.xlarge \\\n",
    "    --instance-count 3 \\\n",
    "    --use-default-roles \\\n",
    "    --region us-east-1 \\\n",
    "    --ec2-attributes '{\"KeyName\": \"vockey\"}' \\\n",
    "    --configurations '[{\"Classification\": \"jupyter-s3-conf\", \"Properties\": {\"s3.persistence.enabled\": \"true\", \"s3.persistence.bucket\": \"rei-example-bucket\"}}]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: `ssh` Directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When creating a new cluster, make sure to adjust the security settings to allow for `ssh` access. See `emr_cheatsheet.md` in Week 7 course materials.\n",
    "\n",
    "2. download the labsuser.pem and save it to your .aws folder\n",
    "3. run this bash command in your terminal: chmod 400 labsuser.pem\n",
    "\n",
    "Then: \n",
    "Connecting to it:\n",
    "```\n",
    "$ ssh -i \"labsuser.pem\" hadoop@ec2-54-197-37-22.compute-1.amazonaws.com\n",
    "\n",
    "Uploading a folder called `mystuff` locally -> EMR:\n",
    "```\n",
    "$ scp -i \"labsuser.pem\" -r mystuff @EMR-PUBLIC-ADDRESS:/home/hadoop\n",
    "```\n",
    "\n",
    "Downloading a folder called `mystuff` from EMR -> locally:\n",
    "```\n",
    "$ scp -i \"labsuser.pem\" -r hadoop@EMR-PUBLIC-ADDRESS:/home/hadoop/mystuff .\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading your files in there, you can then run Spark jobs with\n",
    "``` \n",
    "[EMR] spark-submit mystuff/myfile.py\n",
    "```\n",
    "Alternatively if your files are saved on `S3`, then\n",
    "```\n",
    "[EMR] spark-submit s3://rei-example-bucket/lab_wk7/lab_wk7_spark.py rei-example-bucket\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Interactive Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also launch a Jupyter server directly on EMR and work with it interactively.\n",
    "```\n",
    "$ ssh -i \"labsuser.pem\" -NL 9443:localhost:9443 hadoop@ec2-54-197-37-22.compute-1.amazonaws.com\n",
    "```\n",
    "This forwards the remote connection to your `https://localhost:9443`, and you can log in with username `jovyan`, password `jupyter`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Options: Running Spark on Midway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sbatch`, refer to `in-class-activities/07_Spark/7M_Spark_EDA_ML/midway` on Week 7 course materials. \n",
    "\n",
    "You can also work with Spark interactive on Midway with `sinteractive`:\n",
    "```bash\n",
    "$ sinteractive --time=01:00:00 --nodes=1 --ntasks=10 --mem=40G --partition=caslake --account=macs30113\n",
    "```\n",
    "set up the pyspark environment\n",
    "```bash\n",
    "$ module load python/anaconda-2022.05 spark/3.3.2\n",
    "pyspark --total-executor-cores 9 --executor-memory 4G --driver-memory 4G\n",
    "```\n",
    "log in to your local port\n",
    "```bash\n",
    "$ ssh -NL 8888:10.50.250.12:8888 <your-CNetID>@midway3.rcc.uchicago.edu\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to shut down EMR and clean the bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
