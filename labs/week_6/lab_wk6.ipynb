{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 6\n",
    "\n",
    "## Today's Lab Agenda: \n",
    "\n",
    "1. cd to labs/week_6\n",
    "2. Connect to AWS using terminal \n",
    "3. Set up DynamoDB/sqs/lambda\n",
    "4. Build a Basic AWS Pipeline\n",
    "5. Assignment 6 Overview and Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context: \n",
    "In this week’s lab, we’re continuing to work with our Twitter-related data — but instead of having each message directly trigger a Lambda function like last week, we're introducing a new component: Amazon SQS (Simple Queue Service).\n",
    "\n",
    "\n",
    "### what's new: \n",
    "Last week, we sent individual tweets directly to a Lambda function, which wrote them into a DynamoDB table.\n",
    "\n",
    "This week, we’re improving that design by using an SQS queue as a buffer. We'll send a batch of tweets to the queue first. Then, Lambda will be triggered just once to process all of them together and store the relevant ones in DynamoDB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why SQS?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "aws_lambda = boto3.client('lambda')\n",
    "sqs = boto3.client('sqs')\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Describe in words what is happening in the code block below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2024-04-25 17:27:29.201000-05:00\n"
     ]
    }
   ],
   "source": [
    "table = dynamodb.create_table(\n",
    "    TableName='twitter',\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'uid',\n",
    "            'KeyType': 'HASH'\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'uid',\n",
    "            'AttributeType': 'S'\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 1,\n",
    "        'WriteCapacityUnits': 1\n",
    "    }\n",
    ")\n",
    "\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName='twitter')\n",
    "\n",
    "print(table.item_count)\n",
    "print(table.creation_date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Describe in words what is happening in the code block below (and what the lambda function that you're creating does)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab_wk6.zip', 'rb') as f:\n",
    "    lambda_zip = f.read()\n",
    "\n",
    "try:\n",
    "    response = aws_lambda.create_function(\n",
    "        FunctionName='lab_wk6',\n",
    "        Runtime='python3.9',\n",
    "        Role=role['Role']['Arn'],\n",
    "        Handler='lambda_function.lambda_handler',\n",
    "        Code=dict(ZipFile=lambda_zip),\n",
    "        Timeout=30\n",
    "    )\n",
    "except aws_lambda.exceptions.ResourceConflictException:\n",
    "    response = aws_lambda.update_function_code(\n",
    "        FunctionName='lab_wk6',\n",
    "        ZipFile=lambda_zip\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Describe in words what is happening in the code block below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    queue_url = sqs.create_queue(QueueName='lab_wk6')['QueueUrl']\n",
    "except sqs.exceptions.QueueNameExists:\n",
    "    queue_url = [url for url in sqs.list_queues()['QueueUrls'] if 'lab_wk6' in url][0]\n",
    "    \n",
    "sqs_info = sqs.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['QueueArn'])\n",
    "sqs_arn = sqs_info['Attributes']['QueueArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Describe in words what is happening in the code block below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = aws_lambda.create_event_source_mapping(\n",
    "        EventSourceArn=sqs_arn,\n",
    "        FunctionName='lab_wk6',\n",
    "        Enabled=True,\n",
    "        BatchSize=10\n",
    "    )\n",
    "except aws_lambda.exceptions.ResourceConflictException:\n",
    "    es_id = aws_lambda.list_event_source_mappings(\n",
    "        EventSourceArn=sqs_arn,\n",
    "        FunctionName='lab_wk6'\n",
    "    )['EventSourceMappings'][0]['UUID']\n",
    "    response = aws_lambda.update_event_source_mapping(\n",
    "        UUID=es_id,\n",
    "        FunctionName='lab_wk6',\n",
    "        Enabled=True,\n",
    "        BatchSize=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [Describe in words what is happening in the code block below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MD5OfMessageBody': 'd3c3a03fccaabb647b29e9e6a42c434d', 'MessageId': 'f19b11d7-0bb0-41b5-8138-daff94495fb8', 'ResponseMetadata': {'RequestId': 'df22292d-26aa-5b1a-a401-7789fd851222', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'df22292d-26aa-5b1a-a401-7789fd851222', 'date': 'Thu, 25 Apr 2024 22:27:59 GMT', 'content-type': 'text/xml', 'content-length': '378', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "{'MD5OfMessageBody': 'c881ef5c6dbf10f77d0c25ef970b0c4d', 'MessageId': 'aa3b0dab-56ec-4702-9ef7-81f88636a815', 'ResponseMetadata': {'RequestId': 'fb49ef21-2e3a-501a-bca0-b185f50ae922', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'fb49ef21-2e3a-501a-bca0-b185f50ae922', 'date': 'Thu, 25 Apr 2024 22:28:00 GMT', 'content-type': 'text/xml', 'content-length': '378', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "{'MD5OfMessageBody': '7615a1360a21ac6618a9898736a4efc1', 'MessageId': 'dc8300aa-5c12-424c-8db9-51663a7fe65d', 'ResponseMetadata': {'RequestId': '2f52552e-8a90-5da4-a2f3-0b8a51a1049c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2f52552e-8a90-5da4-a2f3-0b8a51a1049c', 'date': 'Thu, 25 Apr 2024 22:28:01 GMT', 'content-type': 'text/xml', 'content-length': '378', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"username\": \"john_doe\",\n",
    "        \"datetime\": \"04252024120000\",\n",
    "        \"tweet\": \"this is a fun! #uchicago\"\n",
    "    },\n",
    "    {\n",
    "        \"username\": \"jane_doe\",\n",
    "        \"datetime\": \"05152023140100\",\n",
    "        \"tweet\": \"another day, another dollar\"\n",
    "    },\n",
    "    {\n",
    "        \"username\": \"jane_doe\",\n",
    "        \"datetime\": \"05152023140200\",\n",
    "        \"tweet\": \"went to the museum today #uchicago\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for t in test_data:\n",
    "    response = sqs.send_message(QueueUrl=queue_url, MessageBody=json.dumps(t))\n",
    "    print(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Describe in words (or draw a picture of!) what happens in your AWS Architecture when you run the code above.\n",
    "7. [Bonus] Let's imagine that your Lambda workers are not able to process the data in your SQS queue fast enough (you are no longer using the test data above; you have an EC2 instance that is connected to a streaming Twitter API and streaming a large number of messages into your queue). How might you make the above architecture more scalable? Implement your solution using `boto3`.\n",
    "\n",
    "After you're done, be sure to run a teardown script like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Function Deleted\n",
      "SQS Queue Deleted\n",
      "DynamoDB Table Deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete each pipeline component if it still exists:\n",
    "# Lambda\n",
    "try:\n",
    "    aws_lambda.delete_function(FunctionName='lab_wk6')\n",
    "    print(\"Lambda Function Deleted\")\n",
    "except aws_lambda.exceptions.ResourceNotFoundException:\n",
    "    print(\"AWS Lambda Function Already Deleted\")\n",
    "\n",
    "# SQS\n",
    "try:\n",
    "    sqs.delete_queue(QueueUrl=queue_url)\n",
    "    print(\"SQS Queue Deleted\")\n",
    "except sqs.exceptions.QueueDoesNotExist:\n",
    "    print(\"SQS Queue Already Deleted\")\n",
    "\n",
    "# DynamoDB\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "try:\n",
    "    response = dynamodb.delete_table(TableName='twitter')\n",
    "    print(\"DynamoDB Table Deleted\")\n",
    "except dynamodb.exceptions.ResourceNotFoundException:\n",
    "    print(\"DynamoDB Table Already Deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs301x3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
