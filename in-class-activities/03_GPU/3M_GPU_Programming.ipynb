{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1_E7kjUJ6IU_NL2jzGjMIBx2dz4q_nvcz?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Introduction to GPU Programming with CuPy\n",
        "## Estimating Pi with Monte Carlo Simulation\n",
        "\n",
        "In this notebook, we'll walk through how we can use the `cupy` package -- with friendly `numpy`-style syntax -- to accelerate our embarrasingly parallel code using GPUs. Our goal today will be to a estimate a value -- $\\pi$ -- [using Monte Carlo Simulation](https://en.wikipedia.org/wiki/Monte_Carlo_method#Overview) (a classic task for demonstrating HPC speedups and applicable to any numerical estimation task in the social sciences).\n",
        "\n",
        "First taking a look at the hardware we have available to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqQOzobAMTla",
        "outputId": "3cad49fe-181e-4a4f-f569-6d33a88f55e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr  4 21:29:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P8             34W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_272Nn5NU7"
      },
      "source": [
        "Serially, we can solve this problem in `numpy` like so (generating 100k coordinates in a unit square and identifying whether they fall in a unit circle or not):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yEqGmSJ5nTeK"
      },
      "outputs": [],
      "source": [
        "# NumPy Pi Estimation with Monte Carlo Simulation\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "n_runs = 10 ** 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9yPcgxt7-bs",
        "outputId": "dc039f0e-94e1-465a-c04b-154a6adbe974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.74 ms ± 693 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# Simulate Random Coordinates in Unit Square:\n",
        "ran = np.random.uniform(low=-1, high=1, size=(2, n_runs))\n",
        "\n",
        "# Identify Random Coordinates that fall within Unit Circle and count them\n",
        "result = ran[0] ** 2 + ran[1] ** 2 <= 1\n",
        "n_in_circle = np.sum(result)\n",
        "\n",
        "# Compute pi\n",
        "pi = 4 * n_in_circle / n_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KNBM2QC_0Ys"
      },
      "source": [
        "## GPU Programming with `CuPy`\n",
        "\n",
        "A useful approach for working with array data on GPUs is to use `CuPy`, which replicates much of the functionality of `numpy`, but on GPUs. You can take a look at [the documentation](https://docs.cupy.dev/en/stable/user_guide/basic.html) for more detail on all of the GPU functions that are available in CuPy. If you need to write a function that is not supported in CuPy, you can either [define a your own GPU kernel](https://docs.cupy.dev/en/stable/user_guide/kernel.html) (beyond the scope of the class, as this requires some exposure to underlying CUDA syntax). Note that [CuPy arrays are also communicable via mpi4py](https://docs.cupy.dev/en/stable/user_guide/interoperability.html#mpi4py) in a distributed GPU context. Let's import CuPy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "83C_SSyPiWoV"
      },
      "outputs": [],
      "source": [
        "import cupy as cp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8WySv_eC27e"
      },
      "source": [
        "To use CuPy on our GPU, we just need to copy our NumPy arrays over to our GPU as CuPy arrays. Then, we can perform array operations as if we working with NumPy as usual, but accelerated on our GPU. In order to work with the data again on our CPU, we will need to copy it back from the GPU Device to our CPU host. For instance, to compute pi, we might write the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFgl45uNlEUD",
        "outputId": "c14d4da7-db30-44e9-b9f6-eda28fd69e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.42 ms ± 2.37 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "ran = np.random.uniform(low=-1, high =1, size=(2, n_runs)).astype(np.float64)\n",
        "x, y = ran[0], ran[1]\n",
        "\n",
        "# copy x and y to GPU Device\n",
        "x_dev, y_dev = cp.asarray(x), cp.asarray(y)\n",
        "\n",
        "# Perform Computation on GPU Device\n",
        "result_dev = x_dev ** 2 + y_dev ** 2 <= 1\n",
        "\n",
        "# Copy result from GPU Device to CPU Host\n",
        "result = result_dev.get() # alternatively: cp.asnumpy(result_dev)\n",
        "\n",
        "# Perform final computations on CPU Host\n",
        "n_in_circle = np.sum(result)\n",
        "pi = 4 * n_in_circle / n_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDbMYIDMC5Rs"
      },
      "source": [
        "This is actually around the same speed (or even slower) than our standard NumPy CPU solution, though. Why?\n",
        "\n",
        "## Copying data between CPU Host and GPU Device\n",
        "\n",
        "One of the biggest bottlenecks when working with GPUs is the time it takes to copy data back and forth between the CPU Host and GPU Device. We can try to minimize this time, though, by running more operations on the GPU in an attempt to send and receive smaller communications from CPU-to-GPU and GPU-to-CPU.\n",
        "\n",
        "For instance, in this application, we can perform the summation on the GPU..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06KXRiQptIyO",
        "outputId": "8e2e56bc-0916-4c11-f49d-533055fe91db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6 ms ± 49.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "ran = np.random.uniform(low=-1, high =1, size=(2, n_runs)).astype(np.float64)\n",
        "x, y = ran[0], ran[1]\n",
        "\n",
        "# copy x and y to GPU Device\n",
        "x_dev, y_dev = cp.asarray(x), cp.asarray(y)\n",
        "\n",
        "# Perform Computation on GPU Device\n",
        "result_dev = x_dev ** 2 + y_dev ** 2 <= 1\n",
        "\n",
        "# Perform final computations on GPU Host\n",
        "n_in_circle_dev = cp.sum(result_dev)\n",
        "pi_dev = 4 * n_in_circle_dev / n_runs\n",
        "\n",
        "# Copy result from GPU Device to CPU Host\n",
        "pi = pi_dev.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARJkOIj7pOaq"
      },
      "source": [
        "We can also generate random numbers on our GPU in exactly the same way we did in `numpy` (but our data is now on our GPU and doesn't need to be communicated over):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18MxzEW7CDFz",
        "outputId": "3447ff91-d199-425a-f091-8cf1edf8be97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "447 µs ± 77.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "x_dev, y_dev = cp.random.uniform(low=-1, high=1, size=(2, n_runs), dtype=np.float64)\n",
        "\n",
        "# Perform Computation on GPU Device\n",
        "result_dev = x_dev ** 2 + y_dev ** 2 <= 1\n",
        "\n",
        "# Perform final computations on GPU Host\n",
        "n_in_circle_dev = cp.sum(result_dev)\n",
        "pi_dev = 4 * n_in_circle_dev / n_runs\n",
        "\n",
        "# Copy result from GPU Device to CPU Host\n",
        "pi = pi_dev.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8mGt-Yqujo"
      },
      "source": [
        "## Making it worth the cost to send to a GPU: Data Size and Arithmetic Complexity of Parallel Task\n",
        "\n",
        "GPUs are also best served when the tasks they are performing are on large data sizes, or of especially high arithmetic complexity. Otherwise, the cost of sending/receiving data from GPU-to-CPU may not be worth it.\n",
        "\n",
        "Let's increase the number of runs we are simulating here to 100 million and see how our CPU and GPU solutions compare now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sJ2kh-HOrAOz"
      },
      "outputs": [],
      "source": [
        "n_runs = 10 ** 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdlkx5EQq-sL",
        "outputId": "5fab868e-e72e-4c00-9d8f-cc2142b3116a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.73 s ± 212 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# Simulate Random Coordinates in Unit Square:\n",
        "ran = np.random.uniform(low=-1, high=1, size=(2, n_runs))\n",
        "\n",
        "# Identify Random Coordinates that fall within Unit Circle and count them\n",
        "result = ran[0] ** 2 + ran[1] ** 2 <= 1\n",
        "n_in_circle = np.sum(result)\n",
        "\n",
        "# Compute pi\n",
        "pi = 4 * n_in_circle / n_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJYHmM_dqs8l",
        "outputId": "08d52c1a-13f3-4fb4-9ef6-d24a49a60f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "217 ms ± 3.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "x_dev, y_dev = cp.random.uniform(low=-1, high=1, size=(2, n_runs), dtype=np.float64)\n",
        "\n",
        "# Perform Computation on GPU Device\n",
        "result_dev = x_dev ** 2 + y_dev ** 2 <= 1\n",
        "\n",
        "# Perform final computations on GPU Host\n",
        "n_in_circle_dev = cp.sum(result_dev)\n",
        "pi_dev = 4 * n_in_circle_dev / n_runs\n",
        "\n",
        "# Copy result from GPU Device to CPU Host\n",
        "pi = pi_dev.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qh3n7dKp70e"
      },
      "source": [
        "## Higher Precision than needed?\n",
        "\n",
        "Sometimes, the precision of our datatypes is higher than it needs to be. For instance, we've been performing simulations with double precision floats, but may not need this high of precision for our application. Let's see the impact of lowering our precision:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEajy091pN5j",
        "outputId": "87723aec-707e-4de7-dfcf-d8309b85f3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32.9 ms ± 327 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "x_dev, y_dev = cp.random.uniform(low=-1, high=1, size=(2, n_runs), dtype=np.float32)\n",
        "\n",
        "# Perform Computation on GPU Device\n",
        "result_dev = x_dev ** 2 + y_dev ** 2 <= 1\n",
        "\n",
        "# Perform final computations on GPU Host\n",
        "n_in_circle_dev = cp.sum(result_dev)\n",
        "pi_dev = 4 * n_in_circle_dev / n_runs\n",
        "\n",
        "# Copy result from GPU Device to CPU Host\n",
        "pi = pi_dev.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuaibdXZDKfG"
      },
      "source": [
        "## Benchmarking GPU vs. CPU Usage\n",
        "\n",
        "CuPy also conveniently includes a benchmarking function that explicitly times how much work is being done on GPU versus CPU. We can see, for instance, that if we run the same code and `.get()` or result back on our CPU, there is a significant amount of time that is spent on CPU operations (reinforcing the need to minimize data transfer between CPU and GPU, as well as maximize the work that is done on the GPU):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QMNDK2P8tDxG"
      },
      "outputs": [],
      "source": [
        "x_dev, y_dev = cp.random.uniform(low=-1, high=1, size=(2, n_runs), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhDbYq3WMZMv",
        "outputId": "e5e7a7ce-7631-4b01-84d4-c2555afe4cba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cupy_pi_est         :    CPU: 16577.033 us   +/- 164.497 (min: 16245.637 / max: 16771.746) us     GPU-0: 16592.400 us   +/- 164.864 (min: 16261.248 / max: 16788.511) us"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from cupyx.profiler import benchmark\n",
        "\n",
        "def cupy_pi_est(x, y):\n",
        "  # Compute on GPU via CuPy arrays\n",
        "  result = x ** 2 + y ** 2 <= 1\n",
        "  n_in_circle = cp.sum(result)\n",
        "  pi = 4 * n_in_circle / n_runs\n",
        "\n",
        "  # Get pi back to host CPU from GPU\n",
        "  # half of time is spent just getting pi back to host CPU from GPU device\n",
        "  pi_cpu = pi.get()\n",
        "\n",
        "  return pi_cpu\n",
        "\n",
        "benchmark(cupy_pi_est, (x_dev, y_dev), n_repeat=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoFGszNhD3Ny"
      },
      "source": [
        "Whereas if we perform the same operation without bringing the estimated value back to the CPU via `.get()`, our GPU time remains the same, but there is quite a bit less time spent overall (since there is no data transfer time):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_xGgU1TDn5w",
        "outputId": "fa3ea079-40d7-49b9-db39-6e8d98c3fa61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cupy_pi_est         :    CPU:   422.359 us   +/- 62.551 (min:   354.990 / max:   526.592) us     GPU-0: 16332.170 us   +/- 126.365 (min: 16109.247 / max: 16508.928) us"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cupy_pi_est(x, y):\n",
        "  # Compute on GPU via CuPy arrays\n",
        "  result = x ** 2 + y ** 2 <= 1\n",
        "  n_in_circle = cp.sum(result)\n",
        "  pi = 4 * n_in_circle / n_runs\n",
        "\n",
        "  # don't send result back to CPU here for timing purposes\n",
        "  return pi\n",
        "\n",
        "benchmark(cupy_pi_est, (x_dev, y_dev), n_repeat=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
